{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn import linear_model\n",
    "import sklearn.ensemble as en\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "import time\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "xgbc = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=150,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,  \n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "t = []\n",
    "\n",
    "# clf = tree.DecisionTreeClassifier()\n",
    "# clf = linear_model.LogisticRegression(C=10, solver='sag', penalty= 'l2' ,n_jobs=-1)\n",
    "# clf = en.GradientBoostingClassifier(gb)\n",
    "# clf = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "# param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "# tuned_parameters = {}\n",
    "# clf = GridSearchCV(lr, tuned_parameters)\n",
    "# GridSearchCV(cv=None,estimator=LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,penalty='l2', tol=0.0001),param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(\"/home/mohitrbhardwaj/Desktop/kaggle/quora/train1.csv\")\n",
    "test_set = pd.read_csv(\"/home/mohitrbhardwaj/Desktop/kaggle/quora/test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "qid1\n",
      "qid2\n",
      "question1\n",
      "question2\n",
      "is_duplicate\n"
     ]
    }
   ],
   "source": [
    "for x in train_set:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = tfidf_vectorizer.fit_transform(train_set['question1']+train_set['question2'])\n",
    "tar = train_set['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 87010)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape\n",
    "# tar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for x in res:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf.fit(res,tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test_res = tfidf_vectorizer.transform(test_set['question1']+test_set['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain,x,y,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "#         xgb_param = alg.get_xgb_params()\n",
    "        xgb_param = {}\n",
    "        xgb_param['objective'] = 'binary:logistic'\n",
    "        xgb_param['eval_metric'] = 'logloss'\n",
    "        xgb_param['eta'] = 0.02\n",
    "        xgb_param['max_depth'] = 4\n",
    "        xgtrain = xgb.DMatrix(x, y)\n",
    "#         cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "#             metrics='auc', early_stopping_rounds=early_stopping_rounds)\n",
    "#         alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        watchlist = [(xgtrain, 'train')]\n",
    "        bst = xgb.train(xgb_param, xgtrain, 400, watchlist, early_stopping_rounds=5, verbose_eval=10)\n",
    "    #Fit the algorithm on the data\n",
    "#     alg.fit(x, y,eval_metric='logloss')\n",
    "        \n",
    "    #Predict training set:\n",
    "#     dtrain_predictions = alg.predict(x)\n",
    "#     dtrain_predprob = alg.predict_proba(x)[:,1]   \n",
    "    #Print model report:\n",
    "#     print(\"\\nModel Report\")\n",
    "#     print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, dtrain_predictions))\n",
    "#     print(\"log loss (Train): %f\" % metrics.log_loss(y, dtrain_predprob))\n",
    "#                     \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')\n",
    "#     print(feat_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     print(feat_imp)\n",
    "# test_res.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.690625\n",
      "Will train until train-logloss hasn't improved in 5 rounds.\n",
      "[10]\ttrain-logloss:0.670024\n",
      "[20]\ttrain-logloss:0.65472\n",
      "[30]\ttrain-logloss:0.643108\n",
      "[40]\ttrain-logloss:0.634153\n",
      "[50]\ttrain-logloss:0.626837\n",
      "[60]\ttrain-logloss:0.620831\n",
      "[70]\ttrain-logloss:0.615769\n",
      "[80]\ttrain-logloss:0.611394\n",
      "[90]\ttrain-logloss:0.607542\n",
      "[100]\ttrain-logloss:0.604112\n",
      "[110]\ttrain-logloss:0.600911\n",
      "[120]\ttrain-logloss:0.598059\n",
      "[130]\ttrain-logloss:0.595477\n",
      "[140]\ttrain-logloss:0.593198\n",
      "[150]\ttrain-logloss:0.59094\n",
      "[160]\ttrain-logloss:0.588801\n",
      "[170]\ttrain-logloss:0.586907\n",
      "[180]\ttrain-logloss:0.584845\n",
      "[190]\ttrain-logloss:0.583081\n",
      "[200]\ttrain-logloss:0.581437\n",
      "[210]\ttrain-logloss:0.579674\n",
      "[220]\ttrain-logloss:0.577971\n",
      "[230]\ttrain-logloss:0.576512\n",
      "[240]\ttrain-logloss:0.575086\n",
      "[250]\ttrain-logloss:0.573754\n",
      "[260]\ttrain-logloss:0.572384\n",
      "[270]\ttrain-logloss:0.571136\n",
      "[280]\ttrain-logloss:0.569912\n",
      "[290]\ttrain-logloss:0.568797\n",
      "[300]\ttrain-logloss:0.567649\n",
      "[310]\ttrain-logloss:0.566492\n",
      "[320]\ttrain-logloss:0.565401\n",
      "[330]\ttrain-logloss:0.564393\n",
      "[340]\ttrain-logloss:0.563352\n",
      "[350]\ttrain-logloss:0.562425\n",
      "[360]\ttrain-logloss:0.561486\n",
      "[370]\ttrain-logloss:0.560546\n",
      "[380]\ttrain-logloss:0.55964\n",
      "[390]\ttrain-logloss:0.55882\n",
      "515.1375350952148\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "modelfit(xgbc, t,res,tar)\n",
    "stop_time = time.time()\n",
    "print(stop_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = xgbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-98cfec80d38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mlis1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlis1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mohitrbhardwaj/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents, copy)\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'The tfidf vector is not fitted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mohitrbhardwaj/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mohitrbhardwaj/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vocabulary_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mohitrbhardwaj/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "i = 0\n",
    "tot = []\n",
    "for i in range(len(test_set['question1'][:100])):\n",
    "    lis1=tfidf_vectorizer.transform([test_set['question1'][i]])+ tfidf_vectorizer.transform([test_set['question2'][i]])\n",
    "    tot.append(bst.predict_proba(lis1.toarray()))\n",
    "    i=i+1\n",
    "stop_time = time.time()\n",
    "print(stop_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(tot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tot = np.transpose(tot)\n",
    "print(tot[1].shape)\n",
    "xx=np.transpose(tot[1])\n",
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(clf.score(tfidf_vectorizer.transform(train_set['question1'][13001:16000]+train_set['question2'][13001:16000]).toarray(),np.asarray(train_set['is_duplicate'][13001:16000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sol = pd.DataFrame()\n",
    "sol['test_id']=test_set['test_id']\n",
    "print(sol['test_id'].shape)\n",
    "# sol['question1']=test_set['question1'][:11]\n",
    "# sol['question2']=test_set['question2'][:11]\n",
    "print(\"yaha tk ho? gaya\")\n",
    "sol['is_duplicate']=xx\n",
    "print(\"yeh bhi\")\n",
    "sol.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(sol.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
